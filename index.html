<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Multimodal Deep Learning for Improved Disease Diagnosis
    in Ophthalmology">
  <meta name="keywords" content="multimodal ophthalmology">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multimodal Deep Learning for Improved Disease Diagnosis
    in Ophthalmology</title>
  <link rel="icon" href="static/images/eye_care.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multimodal Deep Learning for Improved Disease Diagnosis
              in Ophthalmology</h1>
            </div>
            </div>
            </div>
            </div>
            </section>


            
    <section class="section hero is-light">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
          <h2 class="title is-2">Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images</h2>
            
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://luxuriant0116.github.io/" target="_blank">Lehan Wang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=4iTfHyQAAAAJ&hl=en" target="_blank">Weihang Dai</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://multi-eye.github.io/" target="_blank">Mei Jin</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://multi-eye.github.io/" target="_blank">Chubin Ou</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://xmengli.github.io/" target="_blank">Xiaomeng Li</a><sup>*</sup><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology<br><sup>2</sup>Guangdong Provincial Hospital of Integrated
                      Traditional Chinese and Western Medicine<br><sup>3</sup>Guangdong Weiren Meditech Co., Ltd</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2308.00291" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/xmed-lab/FDDM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                <a href="https://hkustconnect-my.sharepoint.com/personal/lwangdk_connect_ust_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Flwangdk%5Fconnect%5Fust%5Fhk%2FDocuments%2FMultieye%5Fresource%2Ftopconmm%5Fdata%2Ezip&parent=%2Fpersonal%2Flwangdk%5Fconnect%5Fust%5Fhk%2FDocuments%2FMultieye%5Fresource&ga=1" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                <i class="fab fa-database"></i>
                </span>
                <span>Dataset</span>
                </a>
                </span>
                  

            </div>
          </div>

<!-- Paper abstract -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. To address this problem, we propose a novel fundus-enhanced disease-aware distillation model (FDDM), for retinal disease classification from OCT images. Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use. Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities. Experimental results show that our proposed approach outperforms single-modal, multi-modal, and stateof-the-art distillation methods for retinal disease classification.
          </p>
        </div>
      </div>

    </div>
    <div class="column has-text-centered">
    <h2 class="title is-3">Framework</h2>
    </div>
    <div class="content has-text-justified">
      <p>
        An overview of our framework is shown below. Our method is based on class prototype matching, which distills disease-specific features, and class similarity alignment, which distills inter-class relationships.
      </p>
       <!-- Your image here -->
       <img src="static/images/framework_a.png">
       

  </div>
  </div>
</section>

<section class="section hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
      <h2 class="title is-2">MultiEYE: Dataset and Benchmark for OCT-Enhanced Retinal Disease Recognition from Fundus Images</h2>
        
        <div class="is-size-5 publication-authors">
          <!-- Paper authors -->
          <span class="author-block">
            <a href="https://luxuriant0116.github.io/" target="_blank">Lehan Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4iTfHyQAAAAJ&hl=en" target="_blank">Chubin Ou</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://multi-eye.github.io/" target="_blank">Chongchong Qi</a><sup>3</sup>,</span>
                <span class="author-block">
                  <a href="https://multi-eye.github.io/" target="_blank">Lin An</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://multi-eye.github.io/" target="_blank">Mei Jin</a><sup>4</sup>,</span>
                <span class="author-block">
                  <a href="https://multi-eye.github.io/" target="_blank">Xiangbin Kong</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="https://xmengli.github.io/" target="_blank">Xiaomeng Li</a><sup>*</sup><sup>1</sup>
              </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology<br><sup>2</sup>Guangdong Weiren Meditech Co., Ltd<br><sup>3</sup>Yunnan United Vision Innovations Technol-
                  ogy Co., Ltd.<br><sup>4</sup>Guangdong Hospital of Integrated Traditional Chinese and Western Medicine<br><sup>5</sup>The Second People's Hospital of Foshan</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                     <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a href="https://multi-eye.github.io/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/xmed-lab/MultiEYE" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <span class="link-block">
            <a href="https://multi-eye.github.io/" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
            <i class="fab fa-database"></i>
            </span>
            <span>Dataset</span>
            </a>
            </span>
              

        </div>
      </div>

<!-- Paper abstract -->
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Existing multi-modal learning methods on fundus and OCT images mostly require both modalities to be available and strictly paired for training and testing, which appears less practical in clinical scenarios. To expand the scope of clinical applications, we formulate a novel setting, "OCT-enhanced disease recognition from fundus images", that allows for the use of unpaired multi-modal data during the training phase, and relies on the widespread fundus photographs for testing. To benchmark this setting, we present the first large multi-modal multi-class dataset for eye disease diagnosis, MultiEYE, and propose an OCT-assisted Conceptual Distillation Approach (OCT-CoDA), which employs semantically rich concepts to extract disease-related knowledge from OCT images and leverages them into the fundus model. Specifically, we regard the image-concept relation as a link to distill useful knowledge from OCT teacher model to fundus student model, which considerably improves the diagnostic performance based on fundus images and formulates the cross-modal knowledge transfer into an explainable process. Through extensive experiments on the multi-disease classification task, our proposed OCT-CoDA demonstrates remarkable results and interpretability, showing great potential for clinical application. 
      </p>
    </div>
  </div>

</div>
<div class="column has-text-centered">
<h2 class="title is-3">Framework</h2>
</div>
<div class="content has-text-justified">
  <p>
    The Framework of the Proposed OCT-CoDA Method is presented below. The pre-trained OCT model is adopted as the teacher to train the fundus student model. Given a batch of unpaired OCT images and fundus photos, they are fed into separate image encoders to get the extracted features. To implement the conceptual distillation, we first prompt the LLM to generate a concept pool. Secondly, we compute the similarity between image features and concept embeddings for each modality. Finally, the OCT-assisted distillation is performed based on the image-concept similarity. In the inference stage, we input this similarity matrix into a Fully Connected (FC) layer to obtain the prediction score.
  </p>
   <!-- Your image here -->
   <img src="static/images/framework_b.png">
   

</div>
</div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{wang2023fundus,
      title={Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images},
      author={Wang, Lehan and Dai, Weihang and Jin, Mei and Ou, Chubin and Li, Xiaomeng},
      booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
      pages={639--648},
      year={2023},
      organization={Springer}
    }</code></pre>
  </div>
</section>

<!-- End image carousel -->

  </body>
  </html>